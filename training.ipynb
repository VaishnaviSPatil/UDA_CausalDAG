{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class BayesNetDataset(Dataset):\n",
    "    def __init__(self, is_train=True):\n",
    "        if is_train:\n",
    "            self.x = np.load('./data/dag_mat_child3Domains.npz')\n",
    "        else:\n",
    "            self.x = np.load('./data/dag_mat_child3Domains.npz')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return torch.from_numpy(self.x[idx,:,:]).float()\n",
    "\n",
    "def get_bayesnet_dataloaders(length, batch_size):\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(BayesNetDataset(is_train=True), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(BayesNetDataset(is_train=False), batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import ModuleList\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "from dataloaders import *\n",
    "from models import Generator, Discriminator\n",
    "from train import Trainer\n",
    "from eval import *\n",
    "\n",
    "\n",
    "def get_modified_network(network, dist):\n",
    "    np.random.seed(int(time.time()))\n",
    "    old_network = network\n",
    "    network = network.copy()\n",
    "    length = len(network)\n",
    "    for _ in range(dist):\n",
    "        while True:\n",
    "            i = np.random.choice(length)\n",
    "            j = np.random.choice(length)\n",
    "            if i!=j and j in network[i]:\n",
    "                tmp = network[i].copy()\n",
    "                tmp.remove(j)\n",
    "                network[i] = tmp\n",
    "                break\n",
    "        while True:\n",
    "            i = np.random.choice(length)\n",
    "            j = np.random.choice(length)\n",
    "            if i!=j and j not in network[i] and j not in old_network[i]:\n",
    "                tmp = network[i].copy()\n",
    "                tmp.append(j)\n",
    "                network[i] = tmp\n",
    "                break\n",
    "    return network\n",
    "\n",
    "\n",
    "def get_random_network(length, p):\n",
    "    np.random.seed(int(time.time()))\n",
    "    network = []\n",
    "    for i in range(length):\n",
    "        n = [i]\n",
    "        for j in range(length):\n",
    "            if i!=j and np.random.uniform()<p:\n",
    "                n.append(j)\n",
    "        network.append(n)\n",
    "    return network\n",
    "    \n",
    "TRUE_NETWORK = [[0, 7, 8], [1, 0, 7, 8], [2], [3, 2, 4], [4, 2], [5, 1, 7], [6, 5, 7], [7, 8], [8], [9, 7, 8], [10, 7, 8]]\n",
    "WITHOUT_NETWORK = [list(range(len(TRUE_NETWORK)))]\n",
    "MODIFIED_NETWORK = get_modified_network(TRUE_NETWORK, 3)\n",
    "RANDOM_NETWORK = get_random_network(len(TRUE_NETWORK), 17/110)\n",
    "NETWORK_DICT = {\n",
    "    'true': TRUE_NETWORK,\n",
    "    'without': WITHOUT_NETWORK,\n",
    "    'modified': MODIFIED_NETWORK,\n",
    "    'random': RANDOM_NETWORK,\n",
    "}\n",
    "\n",
    "\n",
    "def main(mode, network_types, expid, epochs, lr, batch_size, pretrain_epochs, ntrials):\n",
    "    length = len(TRUE_NETWORK)\n",
    "    network_dict = {}\n",
    "    result_dict = {}\n",
    "    losses_dict = {}\n",
    "    for network_type in network_types:\n",
    "        # Get network\n",
    "        network = NETWORK_DICT[network_type]\n",
    "    \n",
    "        # Obtain models\n",
    "        generator = Generator(length=length)\n",
    "        discriminator = ModuleList([Discriminator(corrlength=len(n)) for n in network])\n",
    "\n",
    "        # Obtain dataloader\n",
    "        data_loader, test_data_loader = get_bayesnet_dataloaders(length=length, batch_size=batch_size)\n",
    "\n",
    "        # Initialize optimizers\n",
    "        G_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(.9, .99))\n",
    "        D_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(.9, .99))\n",
    "        G_scheduler = torch.optim.lr_scheduler.StepLR(G_optimizer, step_size=50, gamma=1.0)\n",
    "\n",
    "        # Set up trainer\n",
    "        trainer = Trainer(mode, network, network_type, generator, discriminator, G_optimizer, D_optimizer, G_scheduler, use_cuda=torch.cuda.is_available())\n",
    "\n",
    "        # Train model\n",
    "        expdesc = \"{}-e{}lr{}bs{}pe{}nt{}-{}\".format(mode, epochs, int(10000*lr), batch_size, pretrain_epochs, ntrials, expid)\n",
    "        trainer.train(data_loader, test_data_loader, expdesc, epochs=epochs, pretrain_epochs=pretrain_epochs)\n",
    "        \n",
    "        # Get result\n",
    "        display.clear_output(wait=True)\n",
    "        generator.eval()\n",
    "        result_dict[network_type] = {}\n",
    "        ess = []\n",
    "        dss = []\n",
    "        for trial in range(ntrials):\n",
    "            it_data = iter(test_data_loader)\n",
    "            sampled_data = [next(it_data).data.numpy() for i in range(8)]\n",
    "            sampled_data = np.concatenate(sampled_data, axis=0).argmax(2)\n",
    "            fixed_latents = Variable(generator.sample_latent(sampled_data.shape[0]))\n",
    "            generated = generator(fixed_latents.cuda(), 0.1).detach().cpu().data.numpy().argmax(2)\n",
    "            ess.append(energy_statistics(sampled_data, generated))\n",
    "            dss.append(discriminative_score(sampled_data, generated, diagnose=False))\n",
    "            print(\"Evaluating... Progress {:.2f}%\".format((trial+1)/ntrials*100), end='\\r')\n",
    "        ess = np.array(ess)\n",
    "        dss = np.array(dss)\n",
    "        result_dict[network_type]['energy_statistics'] = (ess.mean(), ess.std()/np.sqrt(ntrials))\n",
    "        result_dict[network_type]['discriminative_score'] = (dss.mean(), dss.std()/np.sqrt(ntrials))\n",
    "        losses_dict[network_type] = {}\n",
    "        losses_dict[network_type]['wasserstein_loss'] = trainer.losses['D']\n",
    "        losses_dict[network_type]['energy_statistics'] = trainer.losses['energy_statistics']\n",
    "        network_dict[network_type] = network\n",
    "        \n",
    "    display.clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(18, 16))\n",
    "    gs0 = fig.add_gridspec(2, 1)\n",
    "    ax1 = fig.add_subplot(gs0[0])\n",
    "    ax2 = fig.add_subplot(gs0[1])\n",
    "    for network_type in network_types:\n",
    "        loss = losses_dict[network_type]['wasserstein_loss']\n",
    "        ax1.plot(loss, label=network_type.capitalize())\n",
    "    ax1.set_xlabel(\"Iterations\", fontsize=18)\n",
    "    ax1.set_ylabel(\"Wasserstein Estimations\", fontsize=18)\n",
    "    ax1.legend(fontsize=18)\n",
    "    for network_type in network_types:\n",
    "        ax2.plot(losses_dict[network_type]['energy_statistics'], label=network_type.capitalize())\n",
    "    ax2.set_xlabel(\"Iterations\", fontsize=18)\n",
    "    ax2.set_ylabel(\"Energy Statistics\", fontsize=18)\n",
    "    ax2.legend(fontsize=18)\n",
    "    newpath = './results/{}/'.format(expdesc)\n",
    "    fig.savefig(newpath+'Losses.png')\n",
    "    with open(newpath+'Losses.pickle', 'wb') as handle:\n",
    "        pickle.dump(losses_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(newpath+'Networks.pickle', 'wb') as handle:\n",
    "        pickle.dump(network_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(newpath+'Results.pickle', 'wb') as handle:\n",
    "        pickle.dump(result_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pprint.pprint(result_dict, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ModuleList\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad as torch_grad\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import pickle\n",
    "from skimage import img_as_ubyte\n",
    "import time\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from models import Generator, Discriminator\n",
    "from eval import energy_statistics\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, mode, network, expname, generator, discriminator, gen_optimizer, dis_optimizer, gen_scheduler,\n",
    "                 gp_weight=10, critic_iterations=5, print_every=50, use_cuda=True):\n",
    "        assert mode in ['W', 'JS', 'SH', 'KL', 'TV'], 'Invalid mode.'\n",
    "        self.mode = mode\n",
    "        self.network = network\n",
    "        self.expname = expname\n",
    "        self.G = generator\n",
    "        self.G_opt = gen_optimizer\n",
    "        self.G_sch = gen_scheduler\n",
    "        self.D = discriminator\n",
    "        self.D_opt = dis_optimizer\n",
    "        self.losses = {'G': [], 'D': [], 'gradient_norm': [], 'd_real': [], 'd_generated': [], 'energy_statistics': []}\n",
    "        self.num_steps = 0\n",
    "        self.use_cuda = use_cuda\n",
    "        self.gp_weight = gp_weight\n",
    "        self.critic_iterations = critic_iterations\n",
    "        self.print_every = print_every\n",
    "        self.pca = PCA(n_components=2)\n",
    "        if self.use_cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "\n",
    "    def _critic_train_iteration(self, data, latent_samples, tau, pretrain=False):\n",
    "        # Get generated data\n",
    "        generated_data = self.G(latent_samples, tau).detach()\n",
    "\n",
    "        # Calculate probabilities on real and generated data\n",
    "        d_real, d_generated = 0, 0\n",
    "        for idx, n in enumerate(self.network):\n",
    "            d_real += self.D[idx](data[:,n,:])\n",
    "            d_generated += self.D[idx](generated_data[:,n,:])\n",
    "            \n",
    "        # Create total loss and optimize\n",
    "        self.D_opt.zero_grad()\n",
    "        d_loss = self._fdiv_activation(d_real, d_generated)\n",
    "        \n",
    "        # Record loss\n",
    "        if not pretrain:\n",
    "            self.losses['d_real'].append(d_real.mean().data.item())\n",
    "            self.losses['d_generated'].append(d_generated.mean().data.item())\n",
    "            self.losses['D'].append(-d_loss.data.item())        \n",
    "        \n",
    "        if self.mode == 'W':\n",
    "            # Get gradient penalty\n",
    "            gradient_penalty = self._gradient_penalty(data, generated_data)\n",
    "            d_loss += gradient_penalty\n",
    "            \n",
    "        # Optimization\n",
    "        d_loss.backward()\n",
    "        self.D_opt.step()\n",
    "\n",
    "    def _generator_train_iteration(self, data, latent_samples, tau):\n",
    "        # Get generated data\n",
    "        generated_data = self.G(latent_samples, tau)\n",
    "\n",
    "        # Calculate loss and optimize\n",
    "        d_generated = 0\n",
    "        for idx, n in enumerate(self.network):\n",
    "            d_generated += self.D[idx](generated_data[:,n,:])\n",
    "        \n",
    "        self.G_opt.zero_grad()\n",
    "        g_loss = -self._fdiv_activation(torch.zeros_like(d_generated), d_generated)\n",
    "        \n",
    "        # Record loss\n",
    "        self.losses['G'].append(g_loss.data.item())\n",
    "        \n",
    "        # Optimization\n",
    "        g_loss.backward()\n",
    "        self.G_opt.step()\n",
    "        \n",
    "    def _fdiv_activation(self, d_real, d_generated):\n",
    "        # F-divergence Functions\n",
    "        def gf(v):\n",
    "            if self.mode == 'SH':\n",
    "                return 1-torch.exp(-v)\n",
    "            elif self.mode == 'KL':\n",
    "                return v\n",
    "            elif self.mode == 'JS':\n",
    "                return np.log(2)-torch.log(1+torch.exp(-v))\n",
    "            elif self.mode == 'TV':\n",
    "                return torch.tanh(v)/2\n",
    "            elif self.mode == 'W':\n",
    "                return v\n",
    "        def fs(t):\n",
    "            if self.mode == 'SH':\n",
    "                return t/(1-t)\n",
    "            elif self.mode == 'KL':\n",
    "                return torch.exp(t-1)\n",
    "            elif self.mode == 'JS':\n",
    "                return -torch.log(2-torch.exp(t))\n",
    "            elif self.mode == 'TV':\n",
    "                return t\n",
    "            elif self.mode == 'W':\n",
    "                return t\n",
    "        return (fs(gf(d_generated)) - gf(d_real)).mean()\n",
    "            \n",
    "\n",
    "    def _gradient_penalty(self, real_data, generated_data):\n",
    "        batch_size = real_data.size()[0]\n",
    "\n",
    "        # Calculate interpolation\n",
    "        alpha = torch.rand(batch_size, 1, 1)\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "        if self.use_cuda:\n",
    "            alpha = alpha.cuda()\n",
    "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
    "        interpolated = Variable(interpolated, requires_grad=True)\n",
    "        if self.use_cuda:\n",
    "            interpolated = interpolated.cuda()\n",
    "\n",
    "        # Calculate probability of interpolated examples\n",
    "        prob_interpolated = 0\n",
    "        for idx, n in enumerate(self.network):\n",
    "            prob_interpolated += self.D[idx](interpolated[:,n,:])\n",
    "        \n",
    "        # Calculate gradients of probabilities with respect to examples\n",
    "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda() \n",
    "                               if self.use_cuda else torch.ones(prob_interpolated.size()),\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "        # so flatten to easily take norm per example in batch\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data.item())\n",
    "\n",
    "        # Derivatives of the gradient close to 0 can cause problems because of\n",
    "        # the square root, so manually calculate norm and add epsilon\n",
    "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "        # Return gradient penalty\n",
    "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
    "\n",
    "    def _train_epoch(self, data_loader):\n",
    "        for i, data in enumerate(data_loader):\n",
    "            self.num_steps += 1\n",
    "            \n",
    "            batch_size = data.size()[0]\n",
    "            data = Variable(data)\n",
    "            if self.use_cuda:\n",
    "                data = data.cuda()\n",
    "            latent_samples = Variable(self.G.sample_latent(batch_size))\n",
    "            if self.use_cuda:\n",
    "                latent_samples = latent_samples.cuda()\n",
    "            \n",
    "            self._critic_train_iteration(data, latent_samples, self.tau)\n",
    "            # Only update generator every |critic_iterations| iterations\n",
    "            if self.num_steps % self.critic_iterations == 0:\n",
    "                self._generator_train_iteration(data, latent_samples, self.tau)\n",
    "            \n",
    "            # Visualization\n",
    "            if self.num_steps % self.print_every == 0 and self.num_steps > self.critic_iterations:               \n",
    "                display.clear_output(wait=True)\n",
    "                self.fig = plt.figure(figsize=(21, 9))\n",
    "                self.fig.suptitle(\"Network Type: {}, Epoch: {}/{}\".format(self.expname.capitalize(), self.epoch, self.epochs), fontsize=24)\n",
    "                gs0 = self.fig.add_gridspec(1, 2)\n",
    "                gs1 = gs0[0].subgridspec(2, 1)\n",
    "                ax1 = self.fig.add_subplot(gs1[0])\n",
    "                ax2 = self.fig.add_subplot(gs1[1])\n",
    "                ax3 = self.fig.add_subplot(gs0[1])\n",
    "                \n",
    "                ax1.plot(self.losses['D'])\n",
    "                ax1.set_xlabel('Iterations', fontsize=18)\n",
    "                ax1.set_ylabel('Discriminator Loss', fontsize=18)\n",
    "                \n",
    "                self.generated_data = self.G(self.fixed_latents, self.tau).detach().cpu().data.numpy().argmax(2)\n",
    "                self.losses['energy_statistics'].append(energy_statistics(self.sampled_data, self.generated_data))\n",
    "                ax2.plot(self.print_every*np.arange(len(self.losses['energy_statistics'])), self.losses['energy_statistics'])\n",
    "                ax2.set_xlabel('Iterations', fontsize=18)\n",
    "                ax2.set_ylabel('Energy Statistics', fontsize=18)\n",
    "                \n",
    "                generated_dots = self.pca.transform(self.generated_data)\n",
    "                ax3.scatter(self.true_dots[:,0], self.true_dots[:,1])\n",
    "                ax3.scatter(generated_dots[:,0], generated_dots[:,1])\n",
    "                plt.show()\n",
    "                self.training_progress_images.append(img_as_ubyte(self._draw_img_grid(generated_dots)))\n",
    "        self.G_sch.step()\n",
    "        \n",
    "    def _pretrain_epoch(self, data_loader):\n",
    "        for i, data in enumerate(data_loader):            \n",
    "            batch_size = data.size()[0]\n",
    "            data = Variable(data)\n",
    "            if self.use_cuda:\n",
    "                data = data.cuda()\n",
    "            latent_samples = Variable(self.G.sample_latent(batch_size))\n",
    "            if self.use_cuda:\n",
    "                latent_samples = latent_samples.cuda()\n",
    "            self._critic_train_iteration(data, latent_samples, tau=1, pretrain=True)\n",
    "                \n",
    "    def _draw_img_grid(self, generated_dots):\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(self.true_dots[:,0], self.true_dots[:,1])\n",
    "        plt.scatter(generated_dots[:,0], generated_dots[:,1])\n",
    "        plt.title('Epoch {}'.format(self.epoch), fontsize=12)\n",
    "        fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close()\n",
    "        return image\n",
    "\n",
    "    def train(self, data_loader, test_data_loader, expdesc, epochs, pretrain_epochs):\n",
    "        # Fix latents to see how image generation improves during training\n",
    "        it_data = iter(test_data_loader)\n",
    "        sampled_data = [next(it_data).data.numpy() for i in range(8)]\n",
    "        self.sampled_data = np.concatenate(sampled_data, axis=0).argmax(2)\n",
    "        self.fixed_latents = Variable(self.G.sample_latent(self.sampled_data.shape[0]))\n",
    "        if self.use_cuda:\n",
    "            self.fixed_latents = self.fixed_latents.cuda()\n",
    "        self.pca.fit(self.sampled_data)\n",
    "        self.true_dots = self.pca.transform(self.sampled_data)\n",
    "        self.training_progress_images = []\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        for epoch in range(pretrain_epochs):\n",
    "            self._pretrain_epoch(data_loader)\n",
    "            print(\"Pretraining... Progress {:.2f}%\".format((epoch+1)/pretrain_epochs*100), end='\\r')\n",
    "    \n",
    "        self.epochs = epochs\n",
    "        for epoch in range(epochs):\n",
    "            self.epoch = epoch + 1\n",
    "            self.tau = 1 - 0.9*(self.epoch/self.epochs)\n",
    "            self._train_epoch(data_loader)\n",
    "        \n",
    "        self.expdesc = expdesc\n",
    "        newpath = './results/{}/'.format(self.expdesc) \n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        imageio.mimsave(newpath+'{}.gif'.format(self.expname), self.training_progress_images, \n",
    "                        format='GIF', duration=10.0 / len(self.training_progress_images))\n",
    "        with open(newpath+'{}.pickle'.format(self.expname), 'wb') as handle:\n",
    "            pickle.dump(self.losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        self.fig.savefig(newpath+'{}.png'.format(self.expname))\n",
    "        torch.save(self.G.state_dict(), newpath+'{}.pt'.format(self.expname+'_G'))\n",
    "        torch.save(self.D.state_dict(), newpath+'{}.pt'.format(self.expname+'_D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def scatter_plot(real_ebd, fake_ebd, save_path=None):\n",
    "    '''Draw the scatter plot to compare between the real and fake distributions.\n",
    "    \n",
    "    :param (ndarray, float64) read_ebd: 2-dim embedding of the real samples.\n",
    "    :param (ndarray, float64) fake_ebd: 2-dim embedding of the fake samples.\n",
    "    :param (str) save_path: path to save the figure.\n",
    "    '''\n",
    "    # Validation\n",
    "    assert len(real_ebd.shape) == 2 and len(fake_ebd.shape) == 2 and real_ebd.shape[0] == fake_ebd.shape[0] and real_ebd.shape[1] == 2 and fake_ebd.shape[1] == 2, 'Invalid real_ebd and fake_ebd.'\n",
    "    assert save_path is None or isinstance(save_path, str), 'Invalid save_path.'\n",
    "    # Drawing\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.tight_layout(pad=0)\n",
    "    plt.scatter(real_ebd[:,0], real_ebd[:,1], label='Real', alpha=1)\n",
    "    plt.scatter(fake_ebd[:,0], fake_ebd[:,1], label='Fake', alpha=1)\n",
    "    plt.legend(loc='upper left', fontsize=30)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.gca().yaxis.get_offset_text().set_size(16)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
