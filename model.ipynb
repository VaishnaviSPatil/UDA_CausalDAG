{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import torch.nn.utils.spectral_norm as sn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mlp model\n",
    "# num_layers will be a number and num_nodes will be a list\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layer, num_nodes, relu_final=False):\n",
    "        super(MLP, self).__init__()\n",
    "        main = nn.Sequential()\n",
    "        for l in np.arange(num_layer - 1):\n",
    "            main.add_module('linear{0}'.format(l), nn.Linear(num_nodes[l], num_nodes[l + 1]))\n",
    "            if relu_final:\n",
    "                main.add_module('relu{0}'.format(l), nn.ReLU())\n",
    "            else:\n",
    "                if num_layer > 2 and l < num_layer - 2: # 2 layers = linear network, >2 layers, relu net\n",
    "                    main.add_module('relu{0}'.format(l), nn.ReLU())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a MLP generator to generate the z_i's\n",
    "class MLP_Generator(nn.Module):\n",
    "    def __init__(self, z_dim, pax_dim, plabels_dim, noise_dim, num_layer=1, num_nodes=64):\n",
    "        super(MLP_Generator, self).__init__()\n",
    "\n",
    "        self.decoder = MLP(num_layer + 2, [noise_dim + pax_dim + plabels_dim] + [num_nodes]*num_layer + [z_dim])\n",
    "\n",
    "    # pax is parents of x , plabels are the pseudo labels of the domain\n",
    "    def forward(self, noise, pax, plabels, noise_d=None):\n",
    "\n",
    "        input_gen = torch.cat((pax, noise, plabels), axis=1)\n",
    "        output = self.decoder(input_gen)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator + domain predictor (see if they have to be together)\n",
    "\n",
    "class MLP_AuxClassifier(nn.Module):\n",
    "    def __init__(self, z_dim, plabels_dim, do_num, num_layer=1, num_nodes=64):\n",
    "        super(MLP_AuxClassifier, self).__init__()\n",
    "        # classifying zi's into different domains\n",
    "        self.cls = MLP(num_layer + 2, [z_dim] + [num_nodes]*num_layer +[plabels_dim])\n",
    "        \n",
    "        \n",
    "        self.common_net = MLP(num_layer + 1, [z_dim] + [num_nodes]*num_layer, relu_final=True)\n",
    "        \n",
    "        self.aux_c = nn.Linear(num_nodes, cl_num)\n",
    "        self.aux_c_tw = nn.Linear(num_nodes, cl_num)\n",
    "        \n",
    "        self.aux_d = nn.Linear(num_nodes, do_num)\n",
    "        self.aux_d_tw = nn.Linear(num_nodes, do_num)\n",
    "\n",
    "    def forward(self, input0):\n",
    "        input = self.common_net(input0)\n",
    "        output_c = self.aux_c(input)\n",
    "        output_c_tw = self.aux_c_tw(input)\n",
    "        output_d = self.aux_d(input)\n",
    "        output_d_tw = self.aux_d_tw(input)\n",
    "        output_cls = self.cls(input0)\n",
    "        return output_c, output_c_tw, output_d, output_d_tw, output_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic classifier\n",
    "class MLP_Classifier(nn.Module):\n",
    "    def __init__(self, i_dim, cl_num, num_layer=1, num_nodes=64):\n",
    "        super(MLP_Classifier, self).__init__()\n",
    "        self.net = MLP(num_layer + 2, [i_dim] + [num_nodes]*num_layer +[cl_num])\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_c = self.net(input)\n",
    "        return output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an MLP regressor to predict x from z\n",
    "# ask whether the p_labels have to be added too or what?\n",
    "\n",
    "class MLP_Regressor(nn.Module):\n",
    "    def __init__(self, z_dim, num_layer=1, num_nodes=64):\n",
    "        super(MLP_Regressor, self).__init__()\n",
    "        self.net = MLP(num_layer + 2, [z_dim] + [num_nodes]*num_layer +[1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_c = self.net(input)\n",
    "        return output_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph nodes.\n",
    "class Data(object):\n",
    "    def __init__(self, name):\n",
    "        self.__name = name\n",
    "        self.__links = set()\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    @property\n",
    "    def links(self):\n",
    "        return set(self.__links)\n",
    "\n",
    "    def add_link(self, other):\n",
    "        self.__links.add(other)\n",
    "        other.__links.add(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to represent a graph, for topological sort of DAG\n",
    "class Graph:\n",
    "    def __init__(self, vertices):\n",
    "        self.graph = defaultdict(list)  # dictionary containing adjacency List\n",
    "        self.V = vertices  # No. of vertices\n",
    "\n",
    "    # function to add an edge to graph\n",
    "    def addEdge(self, u, v):\n",
    "        self.graph[u].append(v)\n",
    "\n",
    "    # A recursive function used by topologicalSort\n",
    "    def topologicalSortUtil(self, v, visited, stack):\n",
    "\n",
    "        # Mark the current node as visited.\n",
    "        visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        for i in self.graph[v]:\n",
    "            if visited[i] is False:\n",
    "                self.topologicalSortUtil(i, visited, stack)\n",
    "\n",
    "        # Push current vertex to stack which stores result\n",
    "        stack.insert(0, v)\n",
    "\n",
    "    # The function to do Topological Sort. It uses recursive\n",
    "    # topologicalSortUtil()\n",
    "    def topologicalSort(self):\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = [False] * self.V\n",
    "        stack = []\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for i in range(self.V):\n",
    "            if visited[i] is False:\n",
    "                self.topologicalSortUtil(i, visited, stack)\n",
    "\n",
    "        # Return contents of stack\n",
    "        return stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a decoder according to a DAG\n",
    "class DAG_Generator(nn.Module):\n",
    "    def __init__(self, num_vertices, plabels_dim, pax_dim, z_dim, num_layer=1, num_nodes=64, dagMat=None):\n",
    "        super(DAG_Generator, self).__init__()\n",
    "        \n",
    "        # create a dag with the correct number of vertices\n",
    "        dag = Graph(num_vertices)\n",
    "\n",
    "        # from the adjacency matrix add edges to the dag\n",
    "        for i in range(num_vertices):\n",
    "            for j in range(num_vertices):\n",
    "                if dagMat[j, i]:\n",
    "                    dag.addEdge(i, j)\n",
    "\n",
    "        # extract y and d signs\n",
    "        \n",
    "        ###########check this#################\n",
    "        self.yd_sign = dagMat[:, -2:]\n",
    "        dagMat = dagMat[:, :-2]\n",
    "\n",
    "        # topological sort to get all the vertices in order\n",
    "        nodeSort = dag.topologicalSort()\n",
    "        # dim of parents of the node\n",
    "        numInput = dagMat.sum(1)\n",
    "        \n",
    "        # why do we need this? for every vertex we get a different latent\n",
    "        self.dnet = nn.Linear(do_num, do_dim * num_vertices, bias=False)\n",
    "        \n",
    "        self.cnet = nn.Linear(cl_num, cl_dim * num_vertices, bias=False)\n",
    "\n",
    "        # construct generative network according to the dag\n",
    "        nets = nn.ModuleList()\n",
    "        for i in range(num_vertices):\n",
    "            num_nodesIn = int(numInput[i]) + cl_dim + do_dim + z_dim\n",
    "            num_nodes_i = [num_nodesIn] + [num_nodes]*num_layer + [1]\n",
    "            netMB = MLP(num_layer + 2, num_nodes_i)\n",
    "            nets.append(netMB)\n",
    "\n",
    "        # prediction network\n",
    "        self.nets = nets\n",
    "        self.nodeSort = nodeSort\n",
    "        self.nodesA = np.array(range(i_dim)).reshape(i_dim, 1).tolist()\n",
    "        self.i_dim = i_dim\n",
    "        self.i_dimNew = i_dim\n",
    "        self.do_num = do_num\n",
    "        self.cl_num = cl_num\n",
    "        self.cl_dim = cl_dim\n",
    "        self.do_dim = do_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.dagMat = dagMat\n",
    "        self.numInput = numInput\n",
    "        self.is_reg = is_reg\n",
    "        self.ischain = False\n",
    "\n",
    "        # inputs: class indicator, domain indicator, noise, features\n",
    "        # separate forward for each factor\n",
    "    def forward_indep(self, noise, input_c, input_d, input_x, noise_d=None, device='cpu'):\n",
    "        # class parameter network\n",
    "        batch_size = input_c.size(0)\n",
    "        if self.is_reg:\n",
    "            inputs_c = input_c.view(batch_size, 1)\n",
    "        else:\n",
    "            inputs_c = self.cnet(input_c)\n",
    "        if self.prob:\n",
    "            theta = self.mu + torch.mul(torch.log(1+torch.exp(self.sigma)), noise_d)\n",
    "            inputs_d = torch.matmul(input_d, theta)\n",
    "        else:\n",
    "            inputs_d = self.dnet(input_d)\n",
    "\n",
    "        inputs_n = noise\n",
    "        inputs_f = input_x\n",
    "\n",
    "        # create output array\n",
    "        output = torch.zeros((batch_size, len(self.nodeSort)))\n",
    "        output = output.to(device)\n",
    "\n",
    "        # create a network for each module\n",
    "        for i in self.nodeSort:\n",
    "            inputs_pDim = self.numInput[i]\n",
    "            if inputs_pDim > 0:\n",
    "                index = np.argwhere(self.dagMat[i, :])\n",
    "                index = index.flatten()\n",
    "                index = [int(j) for j in index]\n",
    "                inputs_p = inputs_f[:, index] # get the parent data from real data, not fake data!!!\n",
    "            if not self.is_reg:\n",
    "                inputs_ci = inputs_c[:, i*self.cl_dim:(i+1)*self.cl_dim]\n",
    "            else:\n",
    "                inputs_ci = inputs_c\n",
    "            inputs_di = inputs_d[:, i*self.do_dim:(i+1)*self.do_dim]\n",
    "            inputs_ni = inputs_n[:, i*self.z_dim:(i+1)*self.z_dim]\n",
    "            if inputs_pDim > 0:\n",
    "                inputs_i = torch.cat((inputs_ci, inputs_di, inputs_ni, inputs_p), 1)\n",
    "            else:\n",
    "                inputs_i = torch.cat((inputs_ci, inputs_di, inputs_ni), 1)\n",
    "\n",
    "            output[:, i] = self.nets[i](inputs_i).squeeze()\n",
    "\n",
    "        if self.prob:\n",
    "            KL_reg = 1 + torch.log(torch.log(1+torch.exp(self.sigma))**2) - self.mu**2 - torch.log(1+torch.exp(self.sigma))**2\n",
    "            if KL_reg.shape[1] > 1:\n",
    "                KL_reg = KL_reg.sum(axis=1)\n",
    "            return output, -KL_reg\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    # inputs: class indicator, domain indicator, noise\n",
    "    # forward for all factors in a graph\n",
    "    def forward(self, noise, input_c, input_d, device='cpu', noise_d=None):\n",
    "        # class parameter network\n",
    "        batch_size = input_c.size(0)\n",
    "        if self.is_reg:\n",
    "            inputs_c = input_c.view(batch_size, 1)\n",
    "        else:\n",
    "            inputs_c = self.cnet(input_c)\n",
    "        if self.prob:\n",
    "            theta = self.mu + torch.mul(torch.log(1+torch.exp(self.sigma)), noise_d)\n",
    "            inputs_d = torch.matmul(input_d, theta)\n",
    "        else:\n",
    "            inputs_d = self.dnet(input_d)\n",
    "\n",
    "        inputs_n = noise\n",
    "\n",
    "        output = torch.zeros((batch_size, len(self.nodeSort)))\n",
    "        output = output.to(device)\n",
    "\n",
    "        # create a network for each module\n",
    "        for i in self.nodeSort:\n",
    "            inputs_pDim = self.numInput[i]\n",
    "            if inputs_pDim > 0:\n",
    "                index = np.argwhere(self.dagMat[i, :])\n",
    "                index = index.flatten()\n",
    "                index = [int(j) for j in index]\n",
    "                inputs_p = output[:, index]\n",
    "\n",
    "            if not self.is_reg:\n",
    "                inputs_ci = inputs_c[:, i * self.cl_dim:(i + 1) * self.cl_dim]\n",
    "            else:\n",
    "                inputs_ci = inputs_c\n",
    "            inputs_di = inputs_d[:, i * self.do_dim:(i + 1) * self.do_dim]\n",
    "            inputs_ni = inputs_n[:, i * self.z_dim:(i + 1) * self.z_dim]\n",
    "            if inputs_pDim > 0:\n",
    "                inputs_i = torch.cat((inputs_ci, inputs_di, inputs_ni, inputs_p), 1)\n",
    "            else:\n",
    "                inputs_i = torch.cat((inputs_ci, inputs_di, inputs_ni), 1)\n",
    "\n",
    "            output[:, i] = self.nets[i](inputs_i).squeeze()\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer \n",
    "\n",
    "# DA_Infer trainer on a graph, deterministic/probabilistic theta encoder, implemented by joint MMD\n",
    "class DA_Infer_JMMD_DAG(object):\n",
    "    def __init__(self, config):\n",
    "        #num_vertices\n",
    "        input_dim = config['idim']\n",
    "        num_class = config['num_class']\n",
    "        num_domain = config['num_domain']\n",
    "        dim_class = config['dim_y']\n",
    "        dim_domain = config['dim_d']\n",
    "        dim_hidden = config['dim_z']\n",
    "        G_num_layer = config['G_mlp_layers']\n",
    "        G_num_nodes = config['G_mlp_nodes']\n",
    "        D_num_layer = config['D_mlp_layers']\n",
    "        D_num_nodes = config['D_mlp_nodes']\n",
    "        Dec_num_layer = config['Dec_mlp_layers']\n",
    "        Dec_num_nodes = config['Dec_mlp_nodes']\n",
    "        is_reg = config['is_reg']\n",
    "        dag_mat_file = join(config['data_root'], config['dataset'], config['dag_mat_file'])\n",
    "        npzfile = np.load(dag_mat_file)\n",
    "        dag_mat = npzfile['mat']\n",
    "        \n",
    "        isProb = config['estimate'] == 'Bayesian'\n",
    "\n",
    "        # is our model a dag generator or a pdag one?\n",
    "        if config['G_model'] == 'DAG_Generator':\n",
    "            self.gen = DAG_Generator(input_dim, num_class, num_domain, dim_class, dim_domain, dim_hidden, G_num_layer,\n",
    "                                     G_num_nodes, is_reg, dag_mat, prob=isProb)\n",
    "        if config['G_model'] == 'PDAG_Generator':\n",
    "            self.gen = PDAG_Generator(input_dim, num_class, num_domain, dim_class, dim_domain, dim_hidden, G_num_layer,\n",
    "                                     G_num_nodes, is_reg, dag_mat, prob=isProb)\n",
    "            \n",
    "\n",
    "        utils.seed_rng(config['seed'])\n",
    "        \n",
    "        # discriminator (decide what is to be done)\n",
    "        if config['D_model'] == 'MLP_Classifier':\n",
    "            self.dis = MLP_Classifier(input_dim, num_class, D_num_layer, D_num_nodes)\n",
    "            \n",
    "        # decoder to reconstruct x from the latent z_i's\n",
    "        self.dec = MLP_Regressor(z_dim, D_num_layer, D_num_nodes)\n",
    "\n",
    "        # set optimizer\n",
    "        self.gen_opt = torch.optim.Adam(self.gen.parameters(), lr=config['G_lr'],\n",
    "                                        betas=(config['G_B1'], config['G_B2']))\n",
    "        self.dis_opt = torch.optim.Adam(self.dis.parameters(), lr=config['D_lr'],\n",
    "                                        betas=(config['D_B1'], config['D_B2']))\n",
    "\n",
    "        self.aux_loss_func = nn.CrossEntropyLoss()\n",
    "        self.mmd_loss = 0\n",
    "        self.mmd_loss_s = 0\n",
    "        self.mmd_loss_t = 0\n",
    "        self.aux_loss_c = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.gen.to(device)\n",
    "        self.dis.to(device)\n",
    "        \n",
    "    \n",
    "    # separate training of each module, trained on source + target domain together\n",
    "    # what is a module?\n",
    "    def gen_update(self, x_a, y_a, config, state, device='cpu'):\n",
    " \n",
    "        self.gen.zero_grad()\n",
    "        self.dis.zero_grad()\n",
    "        input_dim = config['idim']\n",
    "        dim_domain = config['dim_d']\n",
    "        batch_size = config['batch_size']\n",
    "        noise_dim = config['dim_noise']\n",
    "        z_dim = config['dim_z']\n",
    "        num_domain = config['num_domain']\n",
    "        num_class = config['num_class']\n",
    "        is_reg = config['is_reg']\n",
    "        do_ss = config['do_ss']\n",
    "        \n",
    "        # in our case dim_domain and num_domain are the same\n",
    "        # create one-hot labels as the pseudo lables \n",
    "        \n",
    "        # y first col is the class info, second col is the domain label\n",
    "\n",
    "        # generate random Gaussian noise (input_dim is the number of vertices)\n",
    "        if dim_hidden != 0:\n",
    "            noise = torch.randn((batch_size, dim_hidden * input_dim), device=device)\n",
    "\n",
    "        # create domain labels (is this the class domain?)\n",
    "        if not is_reg:\n",
    "            y_a_onehot = torch.nn.functional.one_hot(y_a[:, 0], num_class).float()\n",
    "        else:\n",
    "            y_a_onehot = y_a[:, 0].view(batch_size, 1)\n",
    "\n",
    "        # create our pseudo_labels\n",
    "        d_onehot = torch.nn.functional.one_hot(y_a[:, 1], num_domain).float()\n",
    "\n",
    "        # do we need y_a_onehot as the input?\n",
    "        if config['estimate'] == 'ML':\n",
    "            fake_x_a = self.gen.forward_indep(noise, y_a_onehot, d_onehot, x_a, device=device)\n",
    "            fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device)\n",
    "            \n",
    "            # not doing this\n",
    "        elif config['estimate'] == 'Bayesian':\n",
    "            noise_d = torch.randn(num_domain, dim_domain * input_dim).to(device)\n",
    "            fake_x_a, KL_reg = self.gen.forward_indep(noise, y_a_onehot, d_onehot, x_a, device=device, noise_d=noise_d)\n",
    "            fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device, noise_d=noise_d)\n",
    "\n",
    "        # sigma for MMD\n",
    "        base_x = config['base_x']\n",
    "        sigma_list = [0.125, 0.25, 0.5, 1]\n",
    "        # sigma_list = [0.25, 0.5, 1]\n",
    "        sigma_listx = [sigma * base_x for sigma in sigma_list]\n",
    "\n",
    "        ids_s = y_a[:, 1] != num_domain - 1\n",
    "        ids_t = y_a[:, 1] == num_domain - 1\n",
    "\n",
    "        # Train mode 0: only use MMD for G\n",
    "        \n",
    "        # create our own train_mode\n",
    "        if config['train_mode'] == 'm0':  # no bp from classifier C to G\n",
    "            output_cf = self.dis(fake_x_a_cls.detach())\n",
    "            aux_loss_c = self.aux_loss_func(output_cf[ids_t], y_a[ids_t, 0])\n",
    "\n",
    "        if config['train_mode'] == 'm1':  # bp from classifier C to G\n",
    "            output_cr = self.dis(x_a[ids_s])\n",
    "            output_cf = self.dis(fake_x_a_cls)\n",
    "            lambda_src = config['SRC_weight']\n",
    "            if state['epoch'] < config['warmup']:\n",
    "                lambda_tar = 0\n",
    "            else:\n",
    "                lambda_tar = config['TAR_weight']\n",
    "            aux_loss_c_src = lambda_src * self.aux_loss_func(output_cr, y_a[ids_s, 0])\n",
    "            aux_loss_c_tar = lambda_tar * self.aux_loss_func(output_cf[ids_t], y_a[ids_t, 0])\n",
    "            aux_loss_c = aux_loss_c_src + aux_loss_c_tar\n",
    "\n",
    "        # this does not have to be changed\n",
    "        # MMD matching for each factor\n",
    "        batch_size_s = len(y_a[ids_s, :])\n",
    "        # batch_size_t = len(y_a[ids_t, :])\n",
    "        errG_s = torch.zeros(len(self.gen.nodeSort), device=device)\n",
    "        # errG_t = torch.zeros(len(self.gen.nodeSort), device=device)\n",
    "\n",
    "        for i in self.gen.nodeSort:\n",
    "            input_pDim = self.gen.numInput[i]\n",
    "            if input_pDim > 0:\n",
    "                if not self.gen.ischain:\n",
    "                    output_dim = 1\n",
    "                    index = np.argwhere(self.gen.dagMat[i, :])\n",
    "                    index = index.flatten()\n",
    "                    index = [int(j) for j in index]\n",
    "                else:\n",
    "                    output_dim = len(self.gen.nodesA[i])\n",
    "                    if output_dim == 1:\n",
    "                        index = np.argwhere(self.gen.dagMat[self.gen.nodesA[i][0], :])\n",
    "                        index = index.flatten()\n",
    "                        index = [int(j) for j in index]\n",
    "                    else:\n",
    "                        index = np.argwhere(self.gen.dagMatNew[i, :])\n",
    "                        index = index.flatten()\n",
    "                        index = [self.gen.nodesA[j] for j in index]\n",
    "                        index = list(itertools.chain.from_iterable(index))\n",
    "                        index = [int(j) for j in index]\n",
    "                input_p = x_a[:, index].view(batch_size, len(index))\n",
    "                errG_s[i] = mix_rbf_mmd2_joint(fake_x_a[ids_s, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "                                             x_a[ids_s, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "                                             y_a_onehot[ids_s], y_a_onehot[ids_s], d_onehot[ids_s],\n",
    "                                             d_onehot[ids_s], input_p[ids_s], input_p[ids_s], sigma_list=sigma_listx)\n",
    "\n",
    "                # errG_t[i] = mix_rbf_mmd2_joint_regress(fake_x_a[ids_t, self.gen.nodesA[i]].view(batch_size_t, output_dim),\n",
    "                #                                      x_a[ids_t, self.gen.nodesA[i]].view(batch_size_t, output_dim),\n",
    "                #                                      input_p[ids_t], input_p[ids_t], sigma_list=sigma_listx, sigma_list1=sigma_listx)\n",
    "            else:\n",
    "                if not self.gen.ischain:\n",
    "                    output_dim = 1\n",
    "                else:\n",
    "                    output_dim = len(self.gen.nodesA[i])\n",
    "                errG_s[i] = mix_rbf_mmd2_joint(fake_x_a[ids_s][:, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "                                             x_a[ids_s][:, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "                                             y_a_onehot[ids_s], y_a_onehot[ids_s], d_onehot[ids_s],\n",
    "                                             d_onehot[ids_s], sigma_list=sigma_listx)\n",
    "                # errG_t[i] = mix_rbf_mmd2(fake_x_a[ids_t][:, self.gen.nodesA[i]].view(batch_size_t, output_dim),\n",
    "                #                        x_a[ids_t][:, self.gen.nodesA[i]].view(batch_size_t, output_dim), sigma_list=sigma_listx)\n",
    "\n",
    "        errG_t = mix_rbf_mmd2(fake_x_a_cls[ids_t], x_a[ids_t], sigma_list=sigma_listx)\n",
    "\n",
    "        errG_s = errG_s.mean()\n",
    "        # errG_t = errG_t.mean()\n",
    "\n",
    "        lambda_c = config['AC_weight']\n",
    "        if config['estimate'] == 'ML':\n",
    "            errG = errG_s + errG_t + lambda_c * aux_loss_c\n",
    "        elif config['estimate'] == 'Bayesian':\n",
    "            errG = errG_s + errG_t + lambda_c * aux_loss_c + torch.dot(1.0 / do_ss.to(device).squeeze(), KL_reg.squeeze())\n",
    "\n",
    "        errG.backward()\n",
    "        self.gen_opt.step()\n",
    "        self.dis_opt.step()\n",
    "        self.mmd_loss = errG\n",
    "        self.mmd_loss_s = errG_s\n",
    "        self.mmd_loss_t = errG_t\n",
    "        self.aux_loss_c = aux_loss_c\n",
    "        \n",
    "    # have a decoder update and training\n",
    "    def dec_update(self, z, x_a, config, state, device = 'cpu'):\n",
    "\n",
    "    # discriminator update and training\n",
    "    def dis_update(self, x_a, y_a, config, state, device='cpu'):\n",
    "        for p in self.dis.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        self.dis.zero_grad()\n",
    "        input_dim = config['idim']\n",
    "        batch_size = config['batch_size']\n",
    "        z_dim = config['dim_z']\n",
    "        noise_dim = config['dim_noise']\n",
    "        dim_domain = config['dim_d']\n",
    "        num_domain = config['num_domain']\n",
    "        num_class = config['num_class']\n",
    "        is_reg = config['is_reg']\n",
    "        do_ss = config['do_ss']\n",
    "\n",
    "        # generate random Gaussian noise\n",
    "        if noise_dim != 0:\n",
    "            noise = torch.randn((batch_size, dim_hidden * input_dim), device=device)\n",
    "\n",
    "        # create domain labels\n",
    "        if not is_reg:\n",
    "            y_a_onehot = torch.nn.functional.one_hot(y_a[:, 0], num_class).float()\n",
    "        else:\n",
    "            y_a_onehot = y_a[:, 0].view(batch_size, 1)\n",
    "\n",
    "        d_onehot = torch.nn.functional.one_hot(y_a[:, 1], num_domain).float()\n",
    "\n",
    "        if config['estimate'] == 'ML':\n",
    "            fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device)\n",
    "        elif config['estimate'] == 'Bayesian':\n",
    "            noise_d = torch.randn(num_domain, dim_domain * input_dim).to(device)\n",
    "            fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device, noise_d=noise_d)\n",
    "\n",
    "        ids_s = y_a[:, 1] != num_domain - 1\n",
    "        ids_t = y_a[:, 1] == num_domain - 1\n",
    "        output_cr = self.dis(x_a[ids_s])\n",
    "        output_cf = self.dis(fake_x_a_cls.detach())\n",
    "        lambda_src = config['SRC_weight']\n",
    "        lambda_tar = config['TAR_weight']\n",
    "        aux_loss_c_src = lambda_src * self.aux_loss_func(output_cr, y_a[ids_s, 0])\n",
    "        aux_loss_c_tar = lambda_tar * self.aux_loss_func(output_cf[ids_t], y_a[ids_t, 0])\n",
    "        aux_loss_c = aux_loss_c_src + aux_loss_c_tar\n",
    "        aux_loss_c.backward()\n",
    "        self.dis_opt.step()\n",
    "        self.aux_loss_c = aux_loss_c\n",
    "\n",
    "    # resume training from the saved weights\n",
    "    def resume(self, snapshot_prefix):\n",
    "        gen_filename = snapshot_prefix + '_gen.pkl'\n",
    "        dis_filename = snapshot_prefix + '_dis.pkl'\n",
    "        state_filename = snapshot_prefix + '_state.pkl'\n",
    "        self.gen.load_state_dict(torch.load(gen_filename))\n",
    "        self.dis.load_state_dict(torch.load(dis_filename))\n",
    "        state_dict = torch.load(state_filename)\n",
    "        print('Resume the model')\n",
    "        return state_dict\n",
    "\n",
    "    # save the model parameters\n",
    "    def save(self, snapshot_prefix, state_dict):\n",
    "        gen_filename = snapshot_prefix + '_gen.pkl'\n",
    "        dis_filename = snapshot_prefix + '_dis.pkl'\n",
    "        state_filename = snapshot_prefix + '_state.pkl'\n",
    "        torch.save(self.gen.state_dict(), gen_filename)\n",
    "        torch.save(self.dis.state_dict(), dis_filename)\n",
    "        torch.save(state_dict, state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
