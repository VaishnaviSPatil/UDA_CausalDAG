{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import torch.nn.utils.spectral_norm as sn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mlp model\n",
    "# num_layers will be a number and num_nodes will be a list\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layer, num_nodes, relu_final=False):\n",
    "        super(MLP, self).__init__()\n",
    "        main = nn.Sequential()\n",
    "        for l in np.arange(num_layer - 1):\n",
    "            main.add_module('linear{0}'.format(l), nn.Linear(num_nodes[l], num_nodes[l + 1]))\n",
    "            if relu_final:\n",
    "                main.add_module('relu{0}'.format(l), nn.ReLU())\n",
    "            else:\n",
    "                if num_layer > 2 and l < num_layer - 2: # 2 layers = linear network, >2 layers, relu net\n",
    "                    main.add_module('relu{0}'.format(l), nn.ReLU())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a MLP generator to generate the z_i's\n",
    "# i_dim is the dimension of z\n",
    "# z_dim is the dimension of noise\n",
    "# do_dim id the one hot encoded pseudo labels for domain label\n",
    "\n",
    "class MLP_Generator(nn.Module):\n",
    "    def __init__(self, i_dim, do_dim, z_dim, num_layer=1, num_nodes=64, is_reg=False, prob=False):\n",
    "        super(MLP_Generator, self).__init__()\n",
    "        self.prob = prob\n",
    "        self.decoder = MLP(num_layer + 2, [z_dim + do_dim] + [num_nodes]*num_layer + [i_dim])\n",
    "        self.is_reg = is_reg\n",
    "\n",
    "    def forward(self, noise, input_d, noise_d=None):\n",
    "\n",
    "        output = self.decoder(torch.cat((output_d, noise), axis=1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator + domain predictor (see if they have to be together) (see if you need this)\n",
    "\n",
    "class MLP_AuxClassifier(nn.Module):\n",
    "    def __init__(self, z_dim, plabels_dim, do_num, num_layer=1, num_nodes=64):\n",
    "        super(MLP_AuxClassifier, self).__init__()\n",
    "        # classifying zi's into different domains\n",
    "        self.cls = MLP(num_layer + 2, [z_dim] + [num_nodes]*num_layer +[plabels_dim])\n",
    "        \n",
    "        \n",
    "        self.common_net = MLP(num_layer + 1, [z_dim] + [num_nodes]*num_layer, relu_final=True)\n",
    "        \n",
    "        self.aux_c = nn.Linear(num_nodes, cl_num)\n",
    "        self.aux_c_tw = nn.Linear(num_nodes, cl_num)\n",
    "        \n",
    "        self.aux_d = nn.Linear(num_nodes, do_num)\n",
    "        self.aux_d_tw = nn.Linear(num_nodes, do_num)\n",
    "\n",
    "    def forward(self, input0):\n",
    "        input = self.common_net(input0)\n",
    "        output_c = self.aux_c(input)\n",
    "        output_c_tw = self.aux_c_tw(input)\n",
    "        output_d = self.aux_d(input)\n",
    "        output_d_tw = self.aux_d_tw(input)\n",
    "        output_cls = self.cls(input0)\n",
    "        return output_c, output_c_tw, output_d, output_d_tw, output_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic classifier (y_classifier to be trained later)\n",
    "class MLP_Classifier(nn.Module):\n",
    "    def __init__(self, i_dim, cl_num, num_layer=1, num_nodes=64):\n",
    "        super(MLP_Classifier, self).__init__()\n",
    "        self.net = MLP(num_layer + 2, [i_dim] + [num_nodes]*num_layer +[cl_num])\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_c = self.net(input)\n",
    "        return output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an MLP regressor to predict x from z\n",
    "# ask whether the p_labels have to be added too or not?\n",
    "# according to Mucong this should be classifier too\n",
    "\n",
    "class MLP_Regressor(nn.Module):\n",
    "    def __init__(self, z_dim, num_layer=1, num_nodes=64):\n",
    "        super(MLP_Regressor, self).__init__()\n",
    "        self.net = MLP(num_layer + 2, [z_dim] + [num_nodes]*num_layer +[1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_c = self.net(input)\n",
    "        return output_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph nodes.\n",
    "class Data(object):\n",
    "    def __init__(self, name):\n",
    "        self.__name = name\n",
    "        self.__links = set()\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    @property\n",
    "    def links(self):\n",
    "        return set(self.__links)\n",
    "\n",
    "    def add_link(self, other):\n",
    "        self.__links.add(other)\n",
    "        other.__links.add(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to represent a graph, for topological sort of DAG\n",
    "class Graph:\n",
    "    def __init__(self, vertices):\n",
    "        self.graph = defaultdict(list)  # dictionary containing adjacency List\n",
    "        self.V = vertices  # No. of vertices\n",
    "\n",
    "    # function to add an edge to graph\n",
    "    def addEdge(self, u, v):\n",
    "        self.graph[u].append(v)\n",
    "\n",
    "    # A recursive function used by topologicalSort\n",
    "    def topologicalSortUtil(self, v, visited, stack):\n",
    "\n",
    "        # Mark the current node as visited.\n",
    "        visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        for i in self.graph[v]:\n",
    "            if visited[i] is False:\n",
    "                self.topologicalSortUtil(i, visited, stack)\n",
    "\n",
    "        # Push current vertex to stack which stores result\n",
    "        stack.insert(0, v)\n",
    "\n",
    "    # The function to do Topological Sort. It uses recursive\n",
    "    # topologicalSortUtil()\n",
    "    def topologicalSort(self):\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = [False] * self.V\n",
    "        stack = []\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for i in range(self.V):\n",
    "            if visited[i] is False:\n",
    "                self.topologicalSortUtil(i, visited, stack)\n",
    "\n",
    "        # Return contents of stack\n",
    "        return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG_Generator(nn.Module):\n",
    "    def __init__(self, num_vertices, do_dim, z_dim, num_layer=1, latent_dim = 4,\n",
    "                 num_nodes=64, is_reg=False, dagMat=None, prob=True):\n",
    "        super(DAG_Generator, self).__init__()\n",
    "        # create a dag\n",
    "        dag = Graph(num_vertices)\n",
    "\n",
    "        for i in range(num_vertices):\n",
    "            for j in range(num_vertices):\n",
    "                if dagMat[j, i]:\n",
    "                    dag.addEdge(i, j)\n",
    "\n",
    "        # extract y and d signs\n",
    "        self.yd_sign = dagMat[:, -2:]\n",
    "        # the diagonal matrix without the domain info and the labels\n",
    "        dagMat = dagMat[:, :-2]\n",
    "\n",
    "        # topologically sort the vertices to give them an ordering\n",
    "        nodeSort = dag.topologicalSort()\n",
    "        # num of parents for each node\n",
    "        numInput = dagMat.sum(1)\n",
    "\n",
    "#         # define class and domain conditional networks\n",
    "#         self.prob = prob\n",
    "#         if prob:\n",
    "#             # VAE posterior parameters, Gaussian\n",
    "#             self.mu = nn.Parameter(torch.zeros(do_num, do_dim * i_dim))\n",
    "#             self.sigma = nn.Parameter(torch.zeros(do_num, do_dim * i_dim))\n",
    "#         else:\n",
    "#             self.dnet = nn.Linear(do_num, do_dim * i_dim, bias=False)\n",
    "#         if not is_reg:\n",
    "#             self.cnet = nn.Linear(cl_num, cl_dim * i_dim, bias=False)\n",
    "\n",
    "        # construct generative network according to the dag\n",
    "        nets = nn.ModuleList()\n",
    "        for i in range(num_vertices):\n",
    "            num_nodesIn = int(numInput[i]) + do_dim + z_dim\n",
    "            num_nodes_i = [num_nodesIn] + [num_nodes]*num_layer + [latent_dim]\n",
    "            netMB = MLP(num_layer + 2, num_nodes_i)\n",
    "            nets.append(netMB)\n",
    "\n",
    "        # prediction network\n",
    "        self.nets = nets\n",
    "        self.nodeSort = nodeSort\n",
    "        self.nodesA = np.array(range(num_vertices)).reshape(num_vertices, 1).tolist()\n",
    "        self.num_vertices = num_vertices\n",
    "        self.do_dim = do_dim\n",
    "        self.z_dim = z_dim               # z is the noise vector \n",
    "        self.latent_dim = latent_dim\n",
    "        self.dagMat = dagMat\n",
    "        self.numInput = numInput\n",
    "        self.is_reg = is_reg\n",
    "        self.ischain = False\n",
    "\n",
    "        # inputs: domain indicator, noise, features\n",
    "        # separate forward for each factor\n",
    "    def forward_indep(self, noise, input_d, input_x, noise_d=None, device='cpu'):\n",
    "        # class parameter network\n",
    "        \n",
    "        # check if this works?\n",
    "        batch_size = noise.size(0)\n",
    "\n",
    "#         if self.prob:\n",
    "#             theta = self.mu + torch.mul(torch.log(1+torch.exp(self.sigma)), noise_d)\n",
    "#             inputs_d = torch.matmul(input_d, theta)\n",
    "#         else:\n",
    "#             inputs_d = self.dnet(input_d)\n",
    "\n",
    "\n",
    "        inputs_n = noise\n",
    "        inputs_f = input_x\n",
    "\n",
    "        # create output array\n",
    "        output = torch.zeros((batch_size, len(self.nodeSort), self.latent_dim))\n",
    "        output = output.to(device)\n",
    "\n",
    "        # create a network for each module\n",
    "        for i in self.nodeSort:\n",
    "            inputs_pDim = self.numInput[i]\n",
    "            if inputs_pDim > 0:\n",
    "                index = np.argwhere(self.dagMat[i, :])\n",
    "                index = index.flatten()\n",
    "                index = [int(j) for j in index]\n",
    "                inputs_p = inputs_f[:, index] # get the parent data from real data, not fake data!!!\n",
    " \n",
    "            # this can be just one dimensional as there are just some pseudo labels\n",
    "            inputs_di = input_d[:, i*self.do_dim:(i+1)*self.do_dim]\n",
    "            inputs_ni = inputs_n[:, i*self.z_dim:(i+1)*self.z_dim]\n",
    "            if inputs_pDim > 0:\n",
    "                inputs_i = torch.cat((inputs_di, inputs_ni, inputs_p), 1)\n",
    "            else:\n",
    "                inputs_i = torch.cat((inputs_di, inputs_ni), 1)\n",
    "\n",
    "            # check this, might be a source of error\n",
    "            output[:, i, :] = self.nets[i](inputs_i)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "    # inputs: domain indicator, noise\n",
    "    # forward for all factors in a graph\n",
    "    def forward(self, noise, input_d, device='cpu', noise_d=None):\n",
    "        # class parameter network\n",
    "        batch_size = noise.size(0)\n",
    "        inputs_d = input_d\n",
    "\n",
    "        inputs_n = noise\n",
    "\n",
    "        output = torch.zeros((batch_size, len(self.nodeSort), self.latent_dim))\n",
    "        output = output.to(device)\n",
    "\n",
    "        # create a network for each module\n",
    "        for i in self.nodeSort:\n",
    "            inputs_pDim = self.numInput[i]\n",
    "            if inputs_pDim > 0:\n",
    "                index = np.argwhere(self.dagMat[i, :])\n",
    "                index = index.flatten()\n",
    "                index = [int(j) for j in index]\n",
    "                inputs_p = output[:, index]\n",
    "\n",
    "            inputs_di = inputs_d[:, i * self.do_dim:(i + 1) * self.do_dim]\n",
    "            inputs_ni = inputs_n[:, i * self.z_dim:(i + 1) * self.z_dim]\n",
    "            if inputs_pDim > 0:\n",
    "                inputs_i = torch.cat((inputs_di, inputs_ni, inputs_p), 1)\n",
    "            else:\n",
    "                inputs_i = torch.cat((inputs_di, inputs_ni), 1)\n",
    "\n",
    "            # check this\n",
    "            output[:, i, :] = self.nets[i](inputs_i)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a dag decoder similar to the dag generator\n",
    "# add a do_dim only if you need the label information in the graph\n",
    "\n",
    "class DAG_Decoder(nn.Module):\n",
    "    def __init__(self, num_vertices, num_layer=1, latent_dim = 4,\n",
    "                 num_nodes=64, is_reg=False, dagMat=None, prob=True, do_dim=None, is_classification=False,\n",
    "                 num_class=2):\n",
    "        super(DAG_Decoder, self).__init__()\n",
    "        \n",
    "        dag = Graph(num_vertices)\n",
    "\n",
    "        for i in range(num_vertices):\n",
    "            for j in range(num_vertices):\n",
    "                if dagMat[j, i]:\n",
    "                    dag.addEdge(i, j)\n",
    "\n",
    "        # extract y and d signs\n",
    "        self.yd_sign = dagMat[:, -2:]       \n",
    "        \n",
    "\n",
    "        dagMat = dagMat[:, :-2]\n",
    "        # topologically sort the vertices to give them an ordering\n",
    "        nodeSort = dag.topologicalSort()\n",
    "        # num of parents for each node\n",
    "        numInput = dagMat.sum(1) # probably don't need it for the decoder\n",
    "\n",
    "        # construct generative network according to the dag\n",
    "        nets = nn.ModuleList()\n",
    "        for i in range(num_vertices):\n",
    "            if do_dim != None:\n",
    "                num_nodesIn = latent_dim + do_dim\n",
    "            else:\n",
    "                num_nodesIn = latent_dim\n",
    "            if is_classification:\n",
    "                num_nodes_i = [num_nodesIn] + [num_nodes]*num_layer + [num_class]\n",
    "            else:\n",
    "                num_nodes_i = [num_nodesIn] + [num_nodes]*num_layer + [1]\n",
    "            netMB = MLP(num_layer + 2, num_nodes_i)\n",
    "            nets.append(netMB)\n",
    "\n",
    "        # prediction network\n",
    "        self.nets = nets\n",
    "        self.nodeSort = nodeSort\n",
    "        self.nodesA = np.array(range(num_vertices)).reshape(num_vertices, 1).tolist()\n",
    "        self.num_vertices = num_vertices\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dagMat = dagMat\n",
    "        self.numInput = numInput # don't think we really need this\n",
    "        self.is_reg = is_reg\n",
    "        self.ischain = False\n",
    "        self.is_classification = is_classification\n",
    "        self.num_class = num_class\n",
    "\n",
    "        # inputs: domain indicator, noise, features\n",
    "        # separate forward for each factor\n",
    "    def forward(self, input_x, device='cpu'):\n",
    "        # class parameter network\n",
    "        \n",
    "        # check if this works?\n",
    "        batch_size = input_x.size(0)\n",
    "\n",
    "#         if self.prob:\n",
    "#             theta = self.mu + torch.mul(torch.log(1+torch.exp(self.sigma)), noise_d)\n",
    "#             inputs_d = torch.matmul(input_d, theta)\n",
    "#         else:\n",
    "#             inputs_d = self.dnet(input_d)\n",
    "\n",
    "        inputs_f = input_x\n",
    "\n",
    "        # create output array\n",
    "        if self.is_classification:\n",
    "            output = torch.zeros((batch_size, len(self.nodeSort), self.num_class))\n",
    "        else:\n",
    "            output = torch.zeros((batch_size, len(self.nodeSort), 1))\n",
    "        output = output.to(device)\n",
    "\n",
    "        # create a network for each module\n",
    "        for i in self.nodeSort:\n",
    "            inputs_p = inputs_f[:, i*self.latent_dim:(i+1)*self.latent_dim]\n",
    "\n",
    "            # check this, might be a source of error\n",
    "            output[:, i, :] = self.nets[i](inputs_p).squeeze()\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer \n",
    "\n",
    "# DA_Infer trainer on a graph, deterministic/probabilistic theta encoder, implemented by joint MMD\n",
    "class DA_Infer_JMMD_DAG(object):\n",
    "    def __init__(self, config):\n",
    "        #num_vertices\n",
    "        input_dim = config['idim']\n",
    "        num_class = config['num_class']\n",
    "        num_domain = config['num_domain']\n",
    "        dim_class = config['dim_y']\n",
    "        dim_hidden = config['dim_z']    # noise dim\n",
    "        latent_dim = config['latent_dim']    # (add this)\n",
    "        G_num_layer = config['G_mlp_layers']\n",
    "        G_num_nodes = config['G_mlp_nodes']\n",
    "        D_num_layer = config['D_mlp_layers']   \n",
    "        D_num_nodes = config['D_mlp_nodes']\n",
    "        Dec_num_layer = config['Dec_mlp_layers'] #(add this)\n",
    "        Dec_num_nodes = config['Dec_mlp_nodes'] # (add this)\n",
    "        is_reg = config['is_reg']\n",
    "        dag_mat_file = join(config['data_root'], config['dataset'], config['dag_mat_file'])\n",
    "        npzfile = np.load(dag_mat_file)\n",
    "        dag_mat = npzfile['mat']\n",
    "        is_classification = config['is_classification'] #(add this)\n",
    "        \n",
    "        isProb = config['estimate'] == 'Bayesian'\n",
    "\n",
    "        # is our model a dag generator or a pdag one?\n",
    "        self.gen = DAG_Generator(input_dim, num_class, num_domain, dim_class, dim_domain, \n",
    "                                     dim_hidden, G_num_layer, latent_dim,\n",
    "                                     G_num_nodes, is_reg, dag_mat, prob=isProb)\n",
    "\n",
    "        utils.seed_rng(config['seed'])\n",
    "        \n",
    "#         # discriminator (decide what is to be done)\n",
    "#         if config['D_model'] == 'MLP_Classifier':\n",
    "#             self.dis = MLP_Classifier(input_dim, num_class, D_num_layer, D_num_nodes)\n",
    "            \n",
    "        # decoder to reconstruct x from the latent z_i's\n",
    "        self.dec = DAG_Decoder(dag, num_layer = Dec_num_layer, latent_dim = latent_dim,\n",
    "                 num_nodes = Dec_num_nodes, is_reg = is_reg, dagMat = dag_mat, prob = isProb, do_dim = num_domain, \n",
    "                 is_classification = is_classification, num_class = num_class)\n",
    "\n",
    "        # set optimizer\n",
    "        self.gen_opt = torch.optim.Adam(self.gen.parameters(), lr=config['G_lr'],\n",
    "                                        betas=(config['G_B1'], config['G_B2']))\n",
    "        self.dec_opt = torch.optim.Adam(self.dec.parameters(), lr=config['Dec_lr'],\n",
    "                                        betas=(config['Dec_B1'], config['Dec_B2'])) #(add this)\n",
    "\n",
    "        self.aux_loss_func = nn.CrossEntropyLoss()\n",
    "        if is_classification:\n",
    "            self.dec_loss_func = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.dec_loss_func = nn.MSELoss()\n",
    "        self.mmd_loss = 0\n",
    "        self.mmd_loss_s = 0\n",
    "        self.mmd_loss_t = 0\n",
    "        self.aux_loss_c = 0\n",
    "        self.dec_loss = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.gen.to(device)\n",
    "        self.dis.to(device)\n",
    "        \n",
    "    \n",
    "    # separate training of each module, trained on source + target domain together\n",
    "    # what is a module?\n",
    "    def gen_update(self, x_a, y_a, config, state, device='cpu'):\n",
    " \n",
    "        self.gen.zero_grad()\n",
    "        self.dec.zero_grad()\n",
    "        input_dim = config['idim']\n",
    "        batch_size = config['batch_size']\n",
    "        z_dim = config['dim_z']\n",
    "        num_domain = config['num_domain']\n",
    "        num_class = config['num_class']\n",
    "        is_reg = config['is_reg']\n",
    "        do_ss = config['do_ss']\n",
    "        \n",
    "        # in our case dim_domain and num_domain are the same\n",
    "        # create one-hot labels as the pseudo lables \n",
    "        \n",
    "        # y first col is the class info, second col is the domain label\n",
    "\n",
    "        # generate random Gaussian noise (input_dim is the number of vertices)\n",
    "        if dim_hidden != 0:\n",
    "            noise = torch.randn((batch_size, dim_hidden * input_dim), device=device)\n",
    "\n",
    "        # create domain labels (is this the class domain?)\n",
    "#         if not is_reg:\n",
    "#             y_a_onehot = torch.nn.functional.one_hot(y_a[:, 0], num_class).float()\n",
    "#         else:\n",
    "#             y_a_onehot = y_a[:, 0].view(batch_size, 1)\n",
    "\n",
    "        # create our pseudo_labels\n",
    "        d_onehot = torch.nn.functional.one_hot(y_a[:, 1], num_domain).float()\n",
    "\n",
    "        # do we need y_a_onehot as the input?\n",
    "        if config['estimate'] == 'ML':\n",
    "            fake_x_a = self.gen.forward_indep(noise, d_onehot, x_a, device=device)\n",
    "            #fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device) # without input x\n",
    "            \n",
    "#             # not doing this\n",
    "#         elif config['estimate'] == 'Bayesian':\n",
    "#             noise_d = torch.randn(num_domain, dim_domain * input_dim).to(device)\n",
    "#             fake_x_a, KL_reg = self.gen.forward_indep(noise, y_a_onehot, d_onehot, x_a, device=device, noise_d=noise_d)\n",
    "#             fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device, noise_d=noise_d)\n",
    "\n",
    "        # sigma for MMD\n",
    "        base_x = config['base_x']\n",
    "        sigma_list = [0.125, 0.25, 0.5, 1]\n",
    "        # sigma_list = [0.25, 0.5, 1]\n",
    "        sigma_listx = [sigma * base_x for sigma in sigma_list]\n",
    "\n",
    "        ids_s = y_a[:, 1] != num_domain - 1\n",
    "        ids_t = y_a[:, 1] == num_domain - 1\n",
    "\n",
    "        # Train mode 0: only use MMD for G\n",
    "        \n",
    "#         # create our own train_mode\n",
    "#         if config['train_mode'] == 'm0':  # no bp from classifier C to G\n",
    "#             output_cf = self.dis(fake_x_a_cls.detach())\n",
    "#             aux_loss_c = self.aux_loss_func(output_cf[ids_t], y_a[ids_t, 0])\n",
    "\n",
    "#         if config['train_mode'] == 'm1':  # bp from classifier C to G\n",
    "#             output_cr = self.dis(x_a[ids_s])\n",
    "#             output_cf = self.dis(fake_x_a_cls)\n",
    "#             lambda_src = config['SRC_weight']\n",
    "#             if state['epoch'] < config['warmup']:\n",
    "#                 lambda_tar = 0\n",
    "#             else:\n",
    "#                 lambda_tar = config['TAR_weight']\n",
    "#             aux_loss_c_src = lambda_src * self.aux_loss_func(output_cr, y_a[ids_s, 0])\n",
    "#             aux_loss_c_tar = lambda_tar * self.aux_loss_func(output_cf[ids_t], y_a[ids_t, 0])\n",
    "#             aux_loss_c = aux_loss_c_src + aux_loss_c_tar\n",
    "\n",
    "        x_recons = self.dec(fake_x_a) # reconstruction using the latent variable \n",
    "        self.dec_loss = (x_recons, x_a)\n",
    "    \n",
    "        # MMD matching for each factor\n",
    "        batch_size_s = len(y_a[ids_s, :])\n",
    "        # batch_size_t = len(y_a[ids_t, :])\n",
    "        errG_s = torch.zeros(len(self.gen.nodeSort), device=device)\n",
    "        # errG_t = torch.zeros(len(self.gen.nodeSort), device=device)\n",
    "\n",
    "#         for i in self.gen.nodeSort:\n",
    "#             input_pDim = self.gen.numInput[i]\n",
    "#             if input_pDim > 0:\n",
    "#                 if not self.gen.ischain:\n",
    "#                     output_dim = 1\n",
    "#                     index = np.argwhere(self.gen.dagMat[i, :])\n",
    "#                     index = index.flatten()\n",
    "#                     index = [int(j) for j in index]\n",
    "#                 else:\n",
    "#                     output_dim = len(self.gen.nodesA[i])\n",
    "#                     if output_dim == 1:\n",
    "#                         index = np.argwhere(self.gen.dagMat[self.gen.nodesA[i][0], :])\n",
    "#                         index = index.flatten()\n",
    "#                         index = [int(j) for j in index]\n",
    "#                     else:\n",
    "#                         index = np.argwhere(self.gen.dagMatNew[i, :])\n",
    "#                         index = index.flatten()\n",
    "#                         index = [self.gen.nodesA[j] for j in index]\n",
    "#                         index = list(itertools.chain.from_iterable(index))\n",
    "#                         index = [int(j) for j in index]\n",
    "#                 input_p = x_a[:, index].view(batch_size, len(index))\n",
    "#                 errG_s[i] = mix_rbf_mmd2_joint(fake_x_a[ids_s, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "#                                              x_a[ids_s, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "#                                              y_a_onehot[ids_s], y_a_onehot[ids_s], d_onehot[ids_s],\n",
    "#                                              d_onehot[ids_s], input_p[ids_s], input_p[ids_s], sigma_list=sigma_listx)\n",
    "\n",
    "#                 # errG_t[i] = mix_rbf_mmd2_joint_regress(fake_x_a[ids_t, self.gen.nodesA[i]].view(batch_size_t, output_dim),\n",
    "#                 #                                      x_a[ids_t, self.gen.nodesA[i]].view(batch_size_t, output_dim),\n",
    "#                 #                                      input_p[ids_t], input_p[ids_t], sigma_list=sigma_listx, sigma_list1=sigma_listx)\n",
    "#             else:\n",
    "#                 if not self.gen.ischain:\n",
    "#                     output_dim = 1\n",
    "#                 else:\n",
    "#                     output_dim = len(self.gen.nodesA[i])\n",
    "#                 errG_s[i] = mix_rbf_mmd2_joint(fake_x_a[ids_s][:, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "#                                              x_a[ids_s][:, self.gen.nodesA[i]].view(batch_size_s, output_dim),\n",
    "#                                              y_a_onehot[ids_s], y_a_onehot[ids_s], d_onehot[ids_s],\n",
    "#                                              d_onehot[ids_s], sigma_list=sigma_listx)\n",
    "#                 # errG_t[i] = mix_rbf_mmd2(fake_x_a[ids_t][:, self.gen.nodesA[i]].view(batch_size_t, output_dim),\n",
    "#                 #                        x_a[ids_t][:, self.gen.nodesA[i]].view(batch_size_t, output_dim), sigma_list=sigma_listx)\n",
    "\n",
    "#         errG_t = mix_rbf_mmd2(fake_x_a_cls[ids_t], x_a[ids_t], sigma_list=sigma_listx)\n",
    "\n",
    "#         errG_s = errG_s.mean()\n",
    "#         # errG_t = errG_t.mean()\n",
    "\n",
    "#         lambda_c = config['AC_weight']\n",
    "#         if config['estimate'] == 'ML':\n",
    "#             errG = errG_s + errG_t + lambda_c * aux_loss_c\n",
    "#         elif config['estimate'] == 'Bayesian':\n",
    "#             errG = errG_s + errG_t + lambda_c * aux_loss_c + torch.dot(1.0 / do_ss.to(device).squeeze(), KL_reg.squeeze())\n",
    "\n",
    "#         errG.backward()\n",
    "\n",
    "        self.dec_loss.backward()\n",
    "        self.gen_opt.step()\n",
    "        self.dec_opt.step()\n",
    "        self.mmd_loss = errG\n",
    "        self.mmd_loss_s = errG_s\n",
    "        self.mmd_loss_t = errG_t\n",
    "        self.aux_loss_c = aux_loss_c\n",
    "        \n",
    "\n",
    "#     # discriminator update and training\n",
    "#     def dis_update(self, x_a, y_a, config, state, device='cpu'):\n",
    "#         for p in self.dis.parameters():\n",
    "#             p.requires_grad_(True)\n",
    "#         self.dis.zero_grad()\n",
    "#         input_dim = config['idim']\n",
    "#         batch_size = config['batch_size']\n",
    "#         z_dim = config['dim_z']\n",
    "#         noise_dim = config['dim_noise']\n",
    "#         dim_domain = config['dim_d']\n",
    "#         num_domain = config['num_domain']\n",
    "#         num_class = config['num_class']\n",
    "#         is_reg = config['is_reg']\n",
    "#         do_ss = config['do_ss']\n",
    "\n",
    "#         # generate random Gaussian noise\n",
    "#         if noise_dim != 0:\n",
    "#             noise = torch.randn((batch_size, dim_hidden * input_dim), device=device)\n",
    "\n",
    "#         # create domain labels\n",
    "#         if not is_reg:\n",
    "#             y_a_onehot = torch.nn.functional.one_hot(y_a[:, 0], num_class).float()\n",
    "#         else:\n",
    "#             y_a_onehot = y_a[:, 0].view(batch_size, 1)\n",
    "\n",
    "#         d_onehot = torch.nn.functional.one_hot(y_a[:, 1], num_domain).float()\n",
    "\n",
    "#         if config['estimate'] == 'ML':\n",
    "#             fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device)\n",
    "#         elif config['estimate'] == 'Bayesian':\n",
    "#             noise_d = torch.randn(num_domain, dim_domain * input_dim).to(device)\n",
    "#             fake_x_a_cls = self.gen(noise, y_a_onehot, d_onehot, device=device, noise_d=noise_d)\n",
    "\n",
    "#         ids_s = y_a[:, 1] != num_domain - 1\n",
    "#         ids_t = y_a[:, 1] == num_domain - 1\n",
    "#         output_cr = self.dis(x_a[ids_s])\n",
    "#         output_cf = self.dis(fake_x_a_cls.detach())\n",
    "#         lambda_src = config['SRC_weight']\n",
    "#         lambda_tar = config['TAR_weight']\n",
    "#         aux_loss_c_src = lambda_src * self.aux_loss_func(output_cr, y_a[ids_s, 0])\n",
    "#         aux_loss_c_tar = lambda_tar * self.aux_loss_func(output_cf[ids_t], y_a[ids_t, 0])\n",
    "#         aux_loss_c = aux_loss_c_src + aux_loss_c_tar\n",
    "#         aux_loss_c.backward()\n",
    "#         self.dis_opt.step()\n",
    "#         self.aux_loss_c = aux_loss_c\n",
    "\n",
    "    # resume training from the saved weights\n",
    "    def resume(self, snapshot_prefix):\n",
    "        gen_filename = snapshot_prefix + '_gen.pkl'\n",
    "        dis_filename = snapshot_prefix + '_dis.pkl'\n",
    "        state_filename = snapshot_prefix + '_state.pkl'\n",
    "        self.gen.load_state_dict(torch.load(gen_filename))\n",
    "        self.dis.load_state_dict(torch.load(dis_filename))\n",
    "        state_dict = torch.load(state_filename)\n",
    "        print('Resume the model')\n",
    "        return state_dict\n",
    "\n",
    "    # save the model parameters\n",
    "    def save(self, snapshot_prefix, state_dict):\n",
    "        gen_filename = snapshot_prefix + '_gen.pkl'\n",
    "        dis_filename = snapshot_prefix + '_dis.pkl'\n",
    "        state_filename = snapshot_prefix + '_state.pkl'\n",
    "        torch.save(self.gen.state_dict(), gen_filename)\n",
    "        torch.save(self.dis.state_dict(), dis_filename)\n",
    "        torch.save(state_dict, state_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
